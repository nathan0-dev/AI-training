{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Fine-Tuning do Phi-3 Mini\n",
    "**Projeto:** Modelo de IA especializado em ensino de programa√ß√£o e Vibe Coding\n",
    "**Modelo base:** Microsoft Phi-3 Mini 4K Instruct\n",
    "**T√©cnica:** LoRA (Low-Rank Adaptation)\n",
    "**Ambiente:** Google Colab (GPU T4 gratuita)\n",
    "\n",
    "---\n",
    "\n",
    "## Estrutura do Projeto\n",
    "1. Instala√ß√£o de depend√™ncias\n",
    "2. Carregamento do modelo base\n",
    "3. Prepara√ß√£o do dataset\n",
    "4. Configura√ß√£o do LoRA\n",
    "5. Treinamento (Fine-Tuning)\n",
    "6. Salvamento do modelo\n",
    "7. Teste e compara√ß√£o (antes vs depois)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Instala√ß√£o de Depend√™ncias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch accelerate bitsandbytes peft trl datasets -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configura√ß√£o do Ambiente\n",
    "> ‚ö†Ô∏è Garanta que a GPU est√° ativa: **Ambiente de execu√ß√£o ‚Üí Alterar ambiente de execu√ß√£o ‚Üí T4 GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.chdir('/content')\n",
    "\n",
    "print('GPU dispon√≠vel:', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU:', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Carregamento do Modelo Base (Phi-3 Mini)\n",
    "Usamos **quantiza√ß√£o 4-bit** para reduzir o uso de mem√≥ria da GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Configura√ß√£o da quantiza√ß√£o 4-bit\n",
    "quantizacao_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    ")\n",
    "\n",
    "model_name = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "\n",
    "print('Baixando tokenizer...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print('Baixando modelo com quantiza√ß√£o 4-bit...')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantizacao_config,\n",
    "    device_map='auto',\n",
    ")\n",
    "\n",
    "print('Modelo carregado com sucesso!')\n",
    "print(f'Mem√≥ria da GPU usada: {torch.cuda.memory_allocated() / 1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Teste do Modelo Base (Antes do Fine-Tuning)\n",
    "Vamos ver como o modelo responde **antes** do treinamento, para comparar depois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversar_base(pergunta: str) -> str:\n",
    "    \"\"\"Gera resposta usando o modelo base (sem fine-tuning).\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Voc√™ √© um assistente especializado em ensino de programa√ß√£o e Vibe Coding. Responda de forma clara e did√°tica.\"},\n",
    "        {\"role\": \"user\", \"content\": pergunta}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.7,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "    resposta = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return resposta\n",
    "\n",
    "\n",
    "pergunta_teste = 'Como funciona um loop for em Python? Me d√° um exemplo simples.'\n",
    "print(f'Pergunta: {pergunta_teste}\\n')\n",
    "print('=' * 50)\n",
    "print('RESPOSTA DO MODELO BASE:\\n')\n",
    "print(conversar_base(pergunta_teste))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Prepara√ß√£o do Dataset\n",
    "> üìÅ Fa√ßa upload dos arquivos `dataset_exemplo.py`, `dataset_completo.py` e `preparar_dataset.py` no Colab antes de rodar essa c√©lula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera dataset.json (3 exemplos base)\n",
    "exec(open('/content/dataset_exemplo.py').read())\n",
    "\n",
    "# Gera dataset_completo.json (17 exemplos)\n",
    "exec(open('/content/dataset_completo.py').read())\n",
    "\n",
    "# Combina e formata tudo no padr√£o do Phi-3\n",
    "exec(open('/content/preparar_dataset.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Classe do Dataset para Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class DatasetFineTuning(Dataset):\n",
    "    \"\"\"Dataset formatado para o fine-tuning do Phi-3.\"\"\"\n",
    "\n",
    "    def __init__(self, caminho: str, tokenizer, max_length: int = 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        with open(caminho, 'r', encoding='utf-8') as f:\n",
    "            self.dados = json.load(f)\n",
    "\n",
    "        print(f'Dataset carregado com {len(self.dados)} exemplos.')\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dados)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        exemplo = self.dados[idx]\n",
    "\n",
    "        texto = self.tokenizer.apply_chat_template(\n",
    "            exemplo['messages'],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "\n",
    "        tokenizado = self.tokenizer(\n",
    "            texto,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = tokenizado['input_ids'].squeeze()\n",
    "        attention_mask = tokenizado['attention_mask'].squeeze()\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[attention_mask == 0] = -100\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels,\n",
    "        }\n",
    "\n",
    "\n",
    "dataset = DatasetFineTuning('/content/dataset_final.json', tokenizer)\n",
    "print(f'Dataset pronto! Exemplos: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Configura√ß√£o do LoRA\n",
    "**LoRA (Low-Rank Adaptation)** permite treinar apenas **0.25%** dos par√¢metros do modelo, tornando o fine-tuning poss√≠vel mesmo em GPUs gratuitas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                                    # Dimens√£o das matrizes LoRA\n",
    "    lora_alpha=32,                           # Escala de aprendizado\n",
    "    target_modules=['qkv_proj', 'o_proj'],   # Camadas ajustadas no Phi-3\n",
    "    lora_dropout=0.05,                       # Regulariza√ß√£o\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM',\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Treinamento (Fine-Tuning)\n",
    "O modelo vai passar pelo dataset 3 vezes (epochs). O **Loss** deve diminuir a cada epoch, indicando aprendizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Configura√ß√µes do treinamento\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 2\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "model.train()\n",
    "\n",
    "print('=== Iniciando Treinamento ===\\n')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        input_ids = batch['input_ids'].to(model.device)\n",
    "        attention_mask = batch['attention_mask'].to(model.device)\n",
    "        labels = batch['labels'].to(model.device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        print(f'  Epoch {epoch+1}/{EPOCHS} | Batch {batch_idx+1}/{len(dataloader)} | Loss: {loss.item():.4f}')\n",
    "\n",
    "    media_loss = total_loss / len(dataloader)\n",
    "    print(f'\\n  Epoch {epoch+1} finalizada | Loss m√©dia: {media_loss:.4f}\\n')\n",
    "\n",
    "print('=== Treinamento Conclu√≠do! ===')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Salvamento do Modelo Treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DIRETORIO_MODELO = '/content/modelo_treinado'\n",
    "os.makedirs(DIRETORIO_MODELO, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(DIRETORIO_MODELO)\n",
    "tokenizer.save_pretrained(DIRETORIO_MODELO)\n",
    "\n",
    "print(f'Modelo salvo em: {DIRETORIO_MODELO}')\n",
    "print('Arquivos salvos:', os.listdir(DIRETORIO_MODELO))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Teste do Modelo Treinado (Ap√≥s Fine-Tuning)\n",
    "Comparando as respostas **antes** e **depois** do treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversar_treinado(pergunta: str) -> str:\n",
    "    \"\"\"Gera resposta usando o modelo ap√≥s fine-tuning.\"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': (\n",
    "                'Voc√™ √© um assistente especializado em ensino de programa√ß√£o e Vibe Coding. '\n",
    "                'Sempre responda com c√≥digo bem estruturado, organizado e seguindo boas pr√°ticas: '\n",
    "                'type hints, docstrings, separa√ß√£o de responsabilidades e coment√°rios explicativos.'\n",
    "            )\n",
    "        },\n",
    "        {'role': 'user', 'content': pergunta}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.7,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "    resposta = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return resposta\n",
    "\n",
    "\n",
    "# Teste 1: Loop for\n",
    "pergunta1 = 'Como funciona um loop for em Python? Me d√° um exemplo simples.'\n",
    "print(f'Pergunta: {pergunta1}\\n')\n",
    "print('=' * 50)\n",
    "print('RESPOSTA DO MODELO TREINADO:\\n')\n",
    "print(conversar_treinado(pergunta1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste 2: Classes\n",
    "pergunta2 = 'Como criar uma classe em Python pra representar um produto?'\n",
    "print(f'Pergunta: {pergunta2}\\n')\n",
    "print('=' * 50)\n",
    "print('RESPOSTA DO MODELO TREINADO:\\n')\n",
    "print(conversar_treinado(pergunta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste 3: Vibe Coding\n",
    "pergunta3 = 'Como eu uso IA pra programar de forma organizada?'\n",
    "print(f'Pergunta: {pergunta3}\\n')\n",
    "print('=' * 50)\n",
    "print('RESPOSTA DO MODELO TREINADO:\\n')\n",
    "print(conversar_treinado(pergunta3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Projeto Conclu√≠do!\n",
    "\n",
    "### Resumo do que foi feito:\n",
    "- Modelo base **Phi-3 Mini** carregado com quantiza√ß√£o 4-bit\n",
    "- Dataset customizado com **20 exemplos** focados em arquitetura e boas pr√°ticas\n",
    "- Fine-tuning usando **LoRA** (apenas 0.25% dos par√¢metros treinados)\n",
    "- Modelo treinado em **3 epochs** com Loss diminuindo consistentemente\n",
    "- Resultados validados: modelo passa a responder com c√≥digo organizado, type hints e docstrings\n",
    "\n",
    "### Tecnologias usadas:\n",
    "- Python | PyTorch | Hugging Face Transformers | PEFT (LoRA) | Google Colab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
