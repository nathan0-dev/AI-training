# ğŸ¤– Fine-Tuning Phi-3 Mini â€” Assistente de ProgramaÃ§Ã£o

Um projeto de Fine-Tuning do modelo **Phi-3 Mini** (Microsoft) para criar um assistente especializado em **ensino de programaÃ§Ã£o** e **Vibe Coding**, que sempre responde com cÃ³digo bem estruturado e seguindo boas prÃ¡ticas.

---

## ğŸ“Œ O que este projeto faz?

Pega o modelo open-source Phi-3 Mini e o ajusta (fine-tuning) para que, ao responder perguntas sobre programaÃ§Ã£o, ele sempre entregue cÃ³digo **organizado, documentado e com boa arquitetura** â€” ao invÃ©s de snippets soltos sem estrutura.

### Antes do Fine-Tuning (modelo base):
```python
for numero in range(1, 6):
    print(numero)
```

### Depois do Fine-Tuning (modelo treinado):
```python
from typing import List

def imprimir_sequencia(inicio: int, fim: int) -> None:
    """
    Imprime uma sequÃªncia de nÃºmeros.

    Args:
        inicio: NÃºmero inicial da sequÃªncia.
        fim: NÃºmero final da sequÃªncia (inclusivo).
    """
    for numero in range(inicio, fim + 1):
        print(numero)


def main():
    imprimir_sequencia(inicio=1, fim=5)


if __name__ == "__main__":
    main()
```

---

## ğŸ—ï¸ Estrutura do Projeto

```
â”œâ”€â”€ fine_tuning_phi3.ipynb      # Notebook completo do projeto (Google Colab)
â”œâ”€â”€ dataset_exemplo.py          # 3 exemplos base do dataset
â”œâ”€â”€ dataset_completo.py         # 17 exemplos por tema
â”œâ”€â”€ preparar_dataset.py         # Script para combinar e formatar o dataset
â””â”€â”€ README.md                   # Este arquivo
```

---

## ğŸ“š Temas Cobertos no Dataset

| Tema | Exemplos |
|------|----------|
| Classes e OOP | 3 |
| Listas e DicionÃ¡rios | 2 |
| Tratamento de Erros | 2 |
| Leitura e Escrita de Arquivos | 2 |
| APIs e RequisiÃ§Ãµes HTTP | 2 |
| Estrutura de DiretÃ³rios e MÃ³dulos | 1 |
| Projetos Pequenos Completos | 2 |
| Vibe Coding | 3 |
| **Total** | **20** |

---

## âš™ï¸ Tecnologias Usadas

- **Python 3.10+**
- **PyTorch** â€” Framework de deep learning
- **Hugging Face Transformers** â€” Carregamento e gerenciamento de modelos
- **PEFT (LoRA)** â€” TÃ©cnica de fine-tuning eficiente
- **BitsAndBytes** â€” QuantizaÃ§Ã£o 4-bit para reduzir uso de memÃ³ria
- **Google Colab** â€” Ambiente de execuÃ§Ã£o com GPU gratuita

---

## ğŸš€ Como Rodar

### 1. PrÃ©-requisitos
- Conta no [Google Colab](https://colab.research.google.com)
- Conta no [Hugging Face](https://huggingface.co)

### 2. Passos

1. FaÃ§a clone do repositÃ³rio:
```bash
git clone https://github.com/SEU_USUARIO/fine-tuning-phi3.git
```

2. Abra o notebook `fine_tuning_phi3.ipynb` no Google Colab

3. Ative a GPU: **Ambiente de execuÃ§Ã£o â†’ Alterar ambiente de execuÃ§Ã£o â†’ T4 GPU**

4. FaÃ§a upload dos arquapÃ³s `.py` no Colab (painel lateral â†’ upload)

5. Rode as cÃ©lulas na ordem â€” o notebook vai guiar todo o processo

---

## ğŸ“Š Resultados do Treinamento

| Epoch | Loss MÃ©dia |
|-------|------------|
| 1 | 0.8715 |
| 2 | 0.5065 |
| 3 | 0.3742 |

O Loss diminuiu consistentemente, confirmando que o modelo aprendeu o padrÃ£o do dataset.

### ParÃ¢metros do LoRA:
- **ParÃ¢metros treinados:** 9,437,184
- **ParÃ¢metros totais:** 3,830,516,736
- **Porcentagem treinada:** 0.25%

---

## ğŸ¯ PrÃ³ximos Passos

- [ ] Aumentar o dataset para 50+ exemplos
- [ ] Adicionar mais temas (testes unitÃ¡rios, design patterns)
- [ ] Criar uma interface web para interagir com o modelo
- [ ] Fazer deploy do modelo em uma API

---

## ğŸ“„ LicenÃ§a

Este projeto foi criado para fins educacionais e de portfÃ³lio.
